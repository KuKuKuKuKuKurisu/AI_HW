{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7845a173",
   "metadata": {},
   "source": [
    "# Conventional ASR: HMM-GMM\n",
    "\n",
    "The objective of this homework is to deepen your understanding of Conventional ASR based on HMM-GMM. There are 3 tasks in this homework.\n",
    "\n",
    "1. **Task 1 [Theoretical, 33%]** Deriving recursion equations for Forward and Viterbi Recursion\n",
    "2. **Task 2 [Theoretical, 33%]** HMM-Gaussians and HMM-GMM\n",
    "3. **Task 3 [Practical, 34%]** Implementation of Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2af2cf",
   "metadata": {},
   "source": [
    "## I. Task 1:\n",
    "\n",
    "As we have learned in class, there are three fundamental problems in HMM:\n",
    "\n",
    "- Problem 1 (Evaluation): Given a sequence of acoustic frames and an HMM model $\\lambda=(A,B)$, we would like to compute $P(O|\\lambda)$ efficiently.\n",
    "- Problem 2 (Decoding): Given a sequence of acoustic features and an HMM model, how to find the state sequence that best explains the observations?\n",
    "- Problem 3 (Learning): Given training data (audios with transcripts), how do we find the best model parameters to maximize $P(O|\\lambda)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd8ff4",
   "metadata": {},
   "source": [
    "**Task 1.a**: In order to solve Problem 1, we can use Forward algorithm with the help of the forward probabilities given as following:\n",
    "\n",
    "$$\\alpha_t(j)=P(o_1,o_2,..., o_t, q_t=j|\\lambda)$$\n",
    "\n",
    "Given the assumptions in HMM, proof that:\n",
    "\n",
    "$$\\alpha_t(j)=\\sum_{i=1}^N\\alpha_{t-1}(i)a_{ij}b_j(o_t)$$\n",
    "\n",
    "where N is the number of states in HMM and $T$ is the lenght of the observation sequence and $1<=j<=N$ and $1<t<=T$. In addition, $a_{ij}$ and $b_j(o_t)$ respectively denotes the transition probability from state i to j and the emission probability of observation $o_t$ given that the state at $t$ is $j$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb1f4f7",
   "metadata": {},
   "source": [
    "**Your answer (written in Markdown)**\n",
    "$$\n",
    "\\alpha_{t+1}(j)=P(o_1,o_2,...,o_t,o_{t+1},q_{t+1}=j|\\lambda)\\\\=\\sum_{i=1}^{N}P(o_1,o_2,...,o_t,o_{t+1},q_{t+1}=j,q_{t}=i|\\lambda)\\\\=\\sum_{i=1}^{N}P(o_{t+1}|o_1,o_2,...,o_t,q_{t+1}=j,q_{t}=i,\\lambda)P(o_1,o_2,...,o_t,q_{t+1}=j,q_{t}=i|\\lambda)\\\\\\because o_{t+1}\\ only\\ depends \\ on \\ q_{t+1}\\\\=\\sum_{i=1}^{N}P(o_{t+1}|q_{t+1}=j,\\lambda)P(o_1,o_2,...,o_t,q_{t+1}=j,q_{t}=i|\\lambda)\\\\=\\sum_{i=1}^{N}P(o_{t+1}|q_{t+1}=j,\\lambda)P(q_{t+1}=j|o_1,o_2,...,o_t,q_t=i,\\lambda)P(o_1,o_2,...,o_t,q_t=i|\\lambda)\\\\\\because q_{t+1} \\ only\\ depends \\ on \\ q_t\\\\=\\sum_{i=1}^{N}P(o_{t+1}|q_{t+1}=j,\\lambda)P(q_{t+1}|q_t,\\lambda)P(o_1,o_2,...,o_t,q_t=i|\\lambda)\\\\=\\sum_{i=1}^{N}a_{ij}b_j(o_{t+1})\\alpha_{t}(i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892c425e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27dea972",
   "metadata": {},
   "source": [
    "**Task 1.b**: In order to solve Problem 2, we can use Viterbi algorithm with the help of the following probabilities:\n",
    "\n",
    "$$v_t(j)=\\max_{q_0,q_1,...,q_{t-1}}P(q_0,q_1,...,q_{t-1},o_1,o_2,..., o_t, q_t=j|\\lambda)$$\n",
    "\n",
    "Given the assumptions in HMM, proof that:\n",
    "\n",
    "$$v_t(j)=\\max_{i=1}^Nv_{t-1}(i)a_{ij}b_j(o_t)$$\n",
    "\n",
    "where N is the number of states in HMM and $T$ is the lenght of the observation sequence and $1<=j<=N$ and $1<t<=T$. In addition, $a_{ij}$ and $b_j(o_t)$ respectively denotes the transition probability from state i to j and the emission probability of observation $o_t$ given that the state at $t$ is $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cf1715",
   "metadata": {},
   "source": [
    "**Your answer (written in Markdown)**\n",
    "$$\n",
    "V_{t+1}(j)=\\max_{q_1,q_2,...,q_t}P(q_1,q_2,...,q_t,o_1,o_2,...,o_{t+1},q_{t+1}=j|\\lambda)\\\\\n",
    "\\because o_{t+1}\\ only\\ depends \\ on \\ q_{t+1}, \\ and \\ q_{t+1} \\ only\\ depends \\ on \\ q_t\\\\\n",
    "=\\max_{i=1}^N\\max_{q_1,q_2,...,q_t}P(q_1,q_2,...,q_{t-1},o_1,o_2,...,o_t,q_t=i|\\lambda)P(q_{t+1}=j|q_t=i,\\lambda)P(o_{t+1}|q_{t+1}=j,\\lambda)\\\\\n",
    "=\\max_{i=1}^NV_t(i)a_{ij}b_j(o_{t+1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855b0bd",
   "metadata": {},
   "source": [
    "## II. Task 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d865f76d",
   "metadata": {},
   "source": [
    "Training HMM can be done using Forward-backward algorithm which interates between E-step for decoding training sequence and M-step for updating model parameters (transition probabilities and emission probabilities). We've learned in class that we can model emission probabilities $b_j(o_t)$ in three ways:\n",
    "\n",
    "#### HMM with Discrete observations: \n",
    "\n",
    "Each observation is assigned to the closest protype from a set of predefined prototypes which can be obtained by clustering. In this case, the emission probability $b_j$ can be updated by the following equation:\n",
    "\n",
    "$$b_j(v_k)=\\frac{\\sum_{r,t,s.t.o^{(r)}_t=v_k}^T\\gamma^{(r)}_t(j)}{\\sum_{r,t}\\gamma^{(r)}_t(j)}$$\n",
    "\n",
    "where $r$ indicates the observation sequence index and $t$ is the observation index in the sequence $r-th$, $\\gamma^{(r)}_t(j)$ indicates the state occupation probability or the probability that the t-th observation of the r-th sequence is at state $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debf4137",
   "metadata": {},
   "source": [
    "#### HMM with Gaussian distributions:\n",
    "\n",
    "Each state $j$ is modeled as Gaussian distribution with mean $\\mu_j$ and variance $\\Sigma_j$. Here, we only use diagonal matrices for the covariance matricies. And suppose that we are given the state ocuppation variables $\\gamma^{(r)}_t(j)$ estimated from the current parameters of HMM, write the equation to update the mean and variance for each state $j$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e68a5d",
   "metadata": {},
   "source": [
    "**Your answer (written in Markdown)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47616e22",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu_j={\\sum_{t,r}\\gamma_t^r(j)o_t^r\\over\\sum_{t,r}\\gamma_t^r(j)}\\\\\n",
    "\\Sigma_j={\\sum_{t,r}\\gamma_t^r(j)(o_t^r-\\mu_j)^2\\over\\sum_{t,r}\\gamma_t^r(j)}\\\\\n",
    "N \\ is \\ number \\ of \\ all \\ observations\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ead2e4",
   "metadata": {},
   "source": [
    "#### HMM with Mixture of Gaussians: \n",
    "\n",
    "Each state $j$ of HMM is modeled by a mixture of a predefined (K) numbers of Gaussians. Before deriving the equations for HMM-GMM updates, let us review some fundamentals of Gaussian Mixture Models.\n",
    "\n",
    "\n",
    "**Gaussian Mixture Model Fundamentals**\n",
    "\n",
    "In a Gaussian Mixture model, data point is assumed to be drawn from a number of Gaussian components. We can think of GMM as the generalization of k-means algorithm where each cluster now is formalized as a Gaussian with mean and covariance, and each point is assigned to a component with a probability instead of hard assignments in K-means. \n",
    "\n",
    "Mathematically, each mixture of (univariate) Gaussians can be described as follows:\n",
    "\n",
    "- K: the number of mixture components\n",
    "- N: the number of observations \n",
    "- $\\mu_l$, $\\sigma_l$ are the mean and covariance matrix for the l-th component (1<=l<=K)\n",
    "- $\\phi$ is the mixture parameter where $\\phi_l$ can be considered the \"weight\" of the l-th component, $\\sum_{l-1}^K\\phi_l=1$\n",
    "\n",
    "Expectation-Maximization can be used to estimate the parameters for GMM model:\n",
    "\n",
    "- **E-step**: With initial guesses for the parameters of our mixture model, \"partial membership\" of each data point in each constituent distribution is computed by calculating expectation values for the membership variables of each data point. That is, for each data point $x_j$, the membership value of observation j-th and the component l-th is $z_{j,l}$:\n",
    "\n",
    "$$z_{j,l}=\\frac{\\phi_lp(x_j|\\mu_l,\\Sigma_l)}{\\sum_k\\phi_kp(x_j|\\mu_k,\\Sigma_k)}$$\n",
    "\n",
    "- **M-step**: Update the parameters of GMM:\n",
    "\n",
    "$$\\hat{\\phi}_l=N_l/N \\text{ where } N_l=\\sum_jz_{j,l}$$\n",
    "\n",
    "$$\\hat{\\mu}_l=\\frac{\\sum_jz_{j,l}x_j}{\\sum_{k,j}z_{j,k}}$$\n",
    "\n",
    "$$\\hat{\\sigma}^2_l=\\frac{\\sum_jz_{j,l}(x_j-\\mu_l)^2}{\\sum_{k,j}z_{j,k}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a5bcde",
   "metadata": {},
   "source": [
    "#### HMM-GMM\n",
    "\n",
    "Back to HMM-GMM, suppose that our observation is described by 1-dimension vector. Furthermore, each state $j$ is modeled by K-component univariate GMM. Based on your understanding of HMM and GMM, write the update equations for parameters $\\mu_{j,l}$, $\\sigma_{j,l}$ and $\\phi_j$ (1<=l<=K) of the state $j$. Please explain the intuition behind those equations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6214e6f6",
   "metadata": {},
   "source": [
    "**Your answer (written in Markdown)**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025162c9",
   "metadata": {},
   "source": [
    "$$\n",
    "E-step:\\\\\n",
    "z_{i,j,l}={\\phi_{j,l}P(x_{i}|\\mu_{j,l},\\Sigma_{j,l})\\over\\sum_k\\phi_{j,k}P(x_{i}|\\mu_{j,k},\\Sigma_{j,k})}\\\\\\\\\n",
    "M-step:\\\\\n",
    "\\hat{\\phi}_{j,l}={N_{j,l}\\over N} \\ where \\ N_{j,l}=\\sum_iz_{i,j,l}\\\\\n",
    "\\hat{\\mu}_{j,l}={\\sum_iz_{i,j,l}x_i\\over\\sum_{k,i}z_{i,j,k}}\\\\\n",
    "\\hat{\\sigma}_{j,l}^2={\\sum_iz_{i,j,l}(x_i-\\mu_{j,l})^2\\over\\sum_{k,i}z_{i,j,k}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d0893",
   "metadata": {},
   "source": [
    "## III. Task 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a5ecb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.000000e+00 0.000000e+00 8.000000e-03 7.200000e-03 6.720000e-03\n",
      "  2.688000e-03 1.075200e-03 8.601600e-04 3.440640e-04 1.548288e-04]\n",
      " [0.000000e+00 4.000000e-02 4.800000e-02 4.480000e-02 1.792000e-02\n",
      "  7.168000e-03 2.867200e-03 8.601600e-04 2.150400e-04 4.300800e-05]\n",
      " [8.000000e-01 3.200000e-01 1.120000e-01 2.240000e-02 4.480000e-03\n",
      "  8.960000e-04 1.792000e-04 4.480000e-05 1.120000e-05 2.800000e-06]]\n",
      "['f', 'f', 'f', 'ay', 'ay', 'ay', 'ay', 'v', 'v', 'v']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def Viterbi(A,B, O):\n",
    "    \"\"\"\n",
    "    @arguments:\n",
    "    A: transition matrix of N*N where A_ij indicates the transition probability from state i to state j\n",
    "    B: emission matrix of size N*V where B_{i,k} indicates \n",
    "       the emission probability of observing k-th prototype given the state i\n",
    "    O: a sequence of observation of length T, O_t \\in [1,...,K]\n",
    "    \n",
    "    @return:\n",
    "    the best state sequence that explains O\n",
    "    \"\"\"\n",
    "    ### BEGIN YOUR CODE ###\n",
    "    path=np.zeros((len(A),len(O)))\n",
    "    v=np.zeros((len(A),len(O)))\n",
    "    pi=np.array([0,0,1])\n",
    "    for i in range(len(O)):\n",
    "        for j in range(len(A)):\n",
    "            for k in range(len(A)):\n",
    "                if i==0:\n",
    "                    v[j][i]=pi[j]*B[j][O[i]]\n",
    "                elif v[k][i-1]*A[k][j]*B[j][O[i]]>v[j][i]:\n",
    "                    v[j][i]=v[k][i-1]*A[k][j]*B[j][O[i]]\n",
    "                    path[j][i]=k\n",
    "    best_sequence=[]\n",
    "    maxstate=v[0][len(O)-1]\n",
    "    state=0\n",
    "    for i in range(len(A)):\n",
    "        if v[i][len(O)-1]>maxstate:\n",
    "            maxstate=v[i][len(O)-1]\n",
    "            state=i\n",
    "    best_sequence.insert(0,state)\n",
    "    for t in range(len(O)-1,0,-1):\n",
    "        state=int(path[state][t])\n",
    "        best_sequence.insert(0,state)\n",
    "    print(v)\n",
    "    ### END YOUR CODE ###\n",
    "    return best_sequence\n",
    "\n",
    "A=[\n",
    "    [0.5,0,0],\n",
    "    [0.5,0.5,0],\n",
    "    [0,0.5,0.5]\n",
    "]\n",
    "\n",
    "B=[\n",
    "    [0.6,0.6,0.4,0.3,0.3,0.3,0.3,0.6,0.8,0.9],\n",
    "    [0.1,0.1,0.3,0.8,0.8,0.8,0.8,0.6,0.5,0.4],\n",
    "    [0.8,0.8,0.7,0.4,0.4,0.4,0.4,0.5,0.5,0.5]\n",
    "]\n",
    "\n",
    "O=[0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "label_dict={\n",
    "    0:\"v\",\n",
    "    1:\"ay\",\n",
    "    2:\"f\"\n",
    "}\n",
    "labels=Viterbi(A,B,O)\n",
    "for i in range(len(labels)):\n",
    "    labels[i]=label_dict[labels[i]]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dfd221",
   "metadata": {},
   "source": [
    "Construct matrices A, B and test your algorithm to explain the results shown on the class:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04364f5",
   "metadata": {},
   "source": [
    "**Viterbi trellis for \"five\"**\n",
    "\n",
    "<img src=\"img/viterbi-a.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d36388",
   "metadata": {},
   "source": [
    "![image-2.png](img/viterbi-b.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e911202b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
