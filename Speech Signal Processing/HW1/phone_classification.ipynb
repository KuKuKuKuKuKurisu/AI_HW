{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d8a9196",
   "metadata": {},
   "source": [
    "# I. Phoneme classification \n",
    "\n",
    "This is the second part of **Homework 1** and it accounts for 60 points of the total score of 100 points for **Homework 1**.\n",
    "\n",
    "In this assignment, you are asked to classify audio segments into 7 phoneme classes (VS, NF, SF, WF, ST, CL and q for glotal ). The meaning of those classes (except q) is given in the following table.\n",
    "\n",
    "\n",
    "![Phoneme classes](image/phoneclass.png)\n",
    "\n",
    "This assignment contains three tasks:\n",
    "\n",
    "    - Task 1: Data Preparation and Feature extraction (10 points)\n",
    "    - Task 2: A simple frame-based classification (35 points)\n",
    "    - Task 3: Written report (15 points)\n",
    "\n",
    "For Task 1, you are asked to use librosa to extract MFCC features from wavfiles. \n",
    "\n",
    "For Task 2, you are given a training dataset and a validation set in the folder audio/part-2. Your method will be evaluated on a hidden test set. Students who can achive results better than our simple baseline can get maximum score of task 2.\n",
    "\n",
    "For Task 3, you are asked to report your experimental results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64c36e",
   "metadata": {},
   "source": [
    "## II. Submission instruction \n",
    "\n",
    "Notes:\n",
    "\n",
    "    1. The notebook will generate python files in submission. The submission folder will need to be uploaded to the course website. \n",
    "    2. While solving the assignment, do **NOT** change class and method names, autograder tests will fail otherwise. However, you can add utility functions into base class if needed.\n",
    "    3. You'll also have to upload a **PDF version** of the notebook (which would be primarily used to grade your report section of the notebook).  \n",
    "    \n",
    "Put the `submission` folder, 2 pdfs exported from 2 notebooks into one folder with name [student_name]_[student_id], zip the folder and upload the course website (the teaching cube)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fe74d9",
   "metadata": {},
   "source": [
    "## III. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55907691",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b256acdd",
   "metadata": {},
   "source": [
    "`submission` contains all the files that you will submit; output contains the output files of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f020ba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try: \n",
    "    os.mkdir('submission')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try:\n",
    "   open('submission/__init__.py', 'x')\n",
    "except FileExistsError:\n",
    "   pass\n",
    "try: \n",
    "    os.mkdir('output')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0683f1c2",
   "metadata": {},
   "source": [
    "## IV. Data Preprocessing and Feature Extraction (10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa80bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission/features.py\n",
    "import glob\n",
    "import pickle\n",
    "import librosa\n",
    "import scipy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def wavfile_to_mfccs(wavfile):\n",
    "    \"\"\"\n",
    "    Returns a matrix of shape (nframes, 39), since there are 39 MFCCs (deltas\n",
    "    included for each 20ms frame in the wavfile).\n",
    "    \"\"\"\n",
    "    x, sampling_rate = librosa.load(wavfile)\n",
    "\n",
    "    window_duration_ms = 20\n",
    "    n_fft = int((window_duration_ms / 1000.) * sampling_rate)\n",
    "\n",
    "    hop_duration_ms = 10\n",
    "    hop_length = int((hop_duration_ms / 1000.) * sampling_rate)\n",
    "\n",
    "    mfcc_count = 13\n",
    "\n",
    "    #### BEGIN YOUR CODE\n",
    "    # Call librosa.feature.mfcc to get mfcc features for each frame of 20ms\n",
    "    # Call librosa.feature.delta on the mfccs to get mfcc_delta\n",
    "    # Call librosa.feature.delta with order 2 on the mfccs to get mfcc_delta2\n",
    "    # Stack all of them (mfcc, mfcc_delta, mfcc_delta2) together \n",
    "    # to get the matrix mfccs_and_deltas of size (#frames, 39)\n",
    "    mfcc=librosa.feature.mfcc(x,sr=sampling_rate,n_mfcc=mfcc_count,n_fft=n_fft,hop_length=hop_length)\n",
    "    mfccs_and_deltas=mfcc\n",
    "    mfcc_delta1=librosa.feature.delta(mfcc,order=1)\n",
    "    mfccs_and_deltas=np.concatenate((mfccs_and_deltas,mfcc_delta1),axis=0)\n",
    "    mfcc_delta2=librosa.feature.delta(mfcc,order=2)\n",
    "    mfccs_and_deltas=np.concatenate((mfccs_and_deltas,mfcc_delta2),axis=0)\n",
    "    mfccs_and_deltas=mfccs_and_deltas.transpose(1,0)\n",
    "    #### END YOUR CODE\n",
    "    return mfccs_and_deltas, hop_length, n_fft\n",
    "\n",
    "class ShortTimeAnalysis:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def perform(self, wavfile):\n",
    "        pass\n",
    "    \n",
    "class MFCCAnalysis(ShortTimeAnalysis):\n",
    "    def perform(self, wavfile):\n",
    "        return wavfile_to_mfccs(wavfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "01ef8a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(394, 39)\n"
     ]
    }
   ],
   "source": [
    "# Testing your method\n",
    "wavfile = \"audio/part-2/train/SA1.WAV\"\n",
    "\n",
    "X, hop_lengt, window_len = wavfile_to_mfccs(wavfile)\n",
    "print (X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "106b0d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission/dataset.py\n",
    "import glob\n",
    "import pickle\n",
    "import librosa\n",
    "import scipy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "\n",
    "unique_classes = ['CL', 'SF', 'VS', 'WF', 'ST', 'NF', \"q\"]\n",
    "\n",
    "def read_data_folder(data_path):\n",
    "    \"\"\"\n",
    "    @return\n",
    "    wav_files: list of file paths to WAV files in the train or val folder.\n",
    "    labels_files: ist of file paths to PHNCLS files in the train or val folder.\n",
    "    \"\"\"\n",
    "    # get all the WAV and PHNCLS in the folder data_path\n",
    "    wav_files = sorted(glob.glob(data_path + \"/*.WAV\"))\n",
    "    labels_files = sorted(glob.glob(data_path + \"/*.PHNCLS\"))\n",
    "    return wav_files, labels_files\n",
    "\n",
    "def extract_features(wavfile, label_file, first_seg_id=0, stanalysis=MFCCAnalysis()):\n",
    "    \"\"\"\n",
    "    Extract segment labels and representations.\n",
    "\n",
    "    @arguments:\n",
    "    wavfile: path to wav file\n",
    "    label_file: path to PHNCLS file\n",
    "    first_seg_id: segment_id of the first segment of the current file.\n",
    "                  When you process a list of files, you may want segment id to increase globally.\n",
    "\n",
    "    @returns:\n",
    "    X: #frames, #features\n",
    "    y: #frames\n",
    "\n",
    "    frame2seg: mapping from frame id to segment id\n",
    "    y_seg: segment labels (segment-based groundtruth)\n",
    "    \"\"\"\n",
    "    X_st, hop_length, window_len = stanalysis.perform(wavfile)\n",
    "\n",
    "    seg_labels = {}\n",
    "    point_seg_ids = []\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            start_frame, end_frame, label = line.split(' ')\n",
    "            start_frame = int(start_frame)\n",
    "            end_frame = int(end_frame)\n",
    "            \n",
    "            label = label.strip()\n",
    "            segment_id = len(seg_labels) + first_seg_id\n",
    "            seg_labels[segment_id] = label\n",
    "            \n",
    "            phn_frames = end_frame - start_frame\n",
    "            point_seg_ids.extend([segment_id]*phn_frames) # point_seg_ids stores segment ids for every sample point.\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    frame_seg_ids = []\n",
    "    curr_frame = curr_hop = 0\n",
    "    while (curr_frame < (len(point_seg_ids) - window_len)):\n",
    "        ### BEGIN YOUR CODE\n",
    "        # extract the segment ids for the sample points within the frame \n",
    "        # from curr_frame to curr_frame + window_len\n",
    "        # Since one frame may overlap with more than one segment, \n",
    "        # sample points within the frame may be assigned with multiple segment ids.\n",
    "        # We get the major segment id as the segment id corresponding to the current frame.\n",
    "        pointer=curr_frame\n",
    "        segment_id=point_seg_ids[curr_frame]\n",
    "        segment_id_count=0\n",
    "        while (pointer<curr_frame+window_len):\n",
    "            if point_seg_ids[pointer]==segment_id:\n",
    "                segment_id_count+=1\n",
    "            else:\n",
    "                segment_id_count-=1\n",
    "                if segment_id_count<0:\n",
    "                    segment_id_count=0\n",
    "                    segment_id=point_seg_ids[pointer]\n",
    "            pointer+=1\n",
    "    \n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        label = seg_labels[segment_id]\n",
    "        y.append(label)\n",
    "        X.append(X_st[curr_hop,:])\n",
    "        frame_seg_ids.append(segment_id)\n",
    "        curr_hop += 1\n",
    "        curr_frame += hop_length\n",
    "\n",
    "    return X, y, frame_seg_ids, seg_labels\n",
    "\n",
    "\n",
    "def prepare_data(wavfiles, label_files, stanalysis=MFCCAnalysis()):\n",
    "    X = []\n",
    "    y = []\n",
    "    segment_ids = []\n",
    "    seg2labels = {}\n",
    "    \n",
    "    file_seg_id = 0\n",
    "    for i in tqdm(range(len(wavfiles))):\n",
    "        wavfile = wavfiles[i]\n",
    "        label_file = label_files[i]\n",
    "        x_, y_, seg_ids_, seg_labels_ = extract_features(\n",
    "            wavfile, label_file, first_seg_id=file_seg_id, stanalysis=stanalysis)\n",
    "\n",
    "        file_seg_id += len(seg_labels_)\n",
    "        for k,v in seg_labels_.items():\n",
    "            seg2labels[k] = v\n",
    "\n",
    "        X.append(x_)\n",
    "        y.extend(y_)\n",
    "        segment_ids.extend(seg_ids_)\n",
    "        \n",
    "\n",
    "    X = np.concatenate(X)\n",
    "    return X, y, segment_ids, seg2labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "36785346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284, 39)\n",
      "284\n",
      "284\n",
      "41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing \n",
    "wavfiles, label_files = read_data_folder(\"audio/part-2/train/\")\n",
    "X, y, segment_ids, seg2labels = prepare_data(wavfiles[0:1], label_files[0:1])\n",
    "print (X.shape)\n",
    "print (len(y))\n",
    "print (len(segment_ids))\n",
    "print (len(seg2labels))\n",
    "assert X.shape[0] == len(y) == len(segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee5017b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile submission/preprocessing.py\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Data normalization\n",
    "def normalize_mean(X):\n",
    "    \"\"\"\n",
    "    Using scikit learn preprocessing to transform feature matrix\n",
    "    using StandardScaler with mean and standard deviation\n",
    "    \"\"\"\n",
    "    ### BEGIN YOUR CODE\n",
    "    scaler=preprocessing.StandardScaler()\n",
    "    X=scaler.fit_transform(X)\n",
    "    ### END YOUR CODE\n",
    "    return X, scaler.mean_, np.sqrt(scaler.var_)\n",
    "\n",
    "def apply_normalize_mean(X, scaler_mean, scaler_std):\n",
    "    \"\"\"\n",
    "    Apply normalizaton to a testing dataset that have been fit using training dataset.\n",
    "    \n",
    "    @arguments:\n",
    "    X: #frames, #features (in case we use mfcc, #features is 39)\n",
    "    scaler_mean: mean of fitted StandardScaler that you used in normalize_mean function.\n",
    "    \n",
    "    @returns:\n",
    "    X: normalized matrix\n",
    "    \"\"\"\n",
    "    ### BEGIN YOUR CODE\n",
    "    X=X-scaler_mean\n",
    "    X=X/scaler_std\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fb1246c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "X, scaler_mean = normalize_mean(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bf7c0a",
   "metadata": {},
   "source": [
    "## V. Phone Classifier (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01653a88",
   "metadata": {},
   "source": [
    "In this part, we will perform isolated phone classification. We assume that phones are well segmented. This section includes two tasks: phone classifier with MFCC, and building your best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ce38b",
   "metadata": {},
   "source": [
    "### Build your best model (10 points)\n",
    "\n",
    "Try different methods to perform feature extraction from frames and report your best result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e4710",
   "metadata": {},
   "source": [
    "### Phone Classifier with MFCC features (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "501219ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submission/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile submission/model.py\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.nn import Sequential\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def onehot_matrix(samples_vec, num_classes):\n",
    "    \"\"\"\n",
    "    >>> onehot_matrix(np.array([1, 0, 3]), 4)\n",
    "    [[ 0.  1.  0.  0.]\n",
    "     [ 1.  0.  0.  0.]\n",
    "     [ 0.  0.  0.  1.]]\n",
    "\n",
    "    >>> onehot_matrix(np.array([2, 2, 0]), 3)\n",
    "    [[ 0.  0.  1.]\n",
    "     [ 0.  0.  1.]\n",
    "     [ 1.  0.  0.]]\n",
    "\n",
    "    Ref: http://bit.ly/1VSKbuc\n",
    "    \"\"\"\n",
    "    num_samples = samples_vec.shape[0]\n",
    "\n",
    "    onehot = np.zeros(shape=(num_samples, num_classes))\n",
    "    onehot[range(0, num_samples), samples_vec] = 1\n",
    "\n",
    "    return onehot\n",
    "\n",
    "def segment_based_evaluation(y_pred, segment_ids, segment2label):\n",
    "    \"\"\"\n",
    "    @argments:\n",
    "    y_pred: predicted labels of frames\n",
    "    segment_ids: segment id of frames\n",
    "    segment2label: mapping from segment id to label\n",
    "    \"\"\"\n",
    "    seg_pred = {}\n",
    "    for frame_id, seg_id in enumerate(segment_ids):\n",
    "        if seg_id not in seg_pred:\n",
    "            seg_pred[seg_id] = []\n",
    "        seg_pred[seg_id].append(y_pred[frame_id])\n",
    "\n",
    "    ncorrect = 0\n",
    "    for seg_id in seg_pred.keys():\n",
    "        predicted = seg_pred[seg_id]\n",
    "        c = Counter(predicted)\n",
    "        predicted_label = c.most_common()[0][0] # take the majority voting\n",
    "\n",
    "        if predicted_label == segment2label[seg_id]:\n",
    "            ncorrect += 1\n",
    "\n",
    "    accuracy = ncorrect/len(segment2label)\n",
    "    print('Segment-based Accuracy using %d testing samples: %f' % (len(segment2label), accuracy))\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.lstm=nn.LSTM(input_size=3,hidden_size=64,batch_first=True)\n",
    "        self.fcmodel=Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=64,out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=64,out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=32,out_features=7)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x,(hn,cn)=self.lstm(x)\n",
    "        x=x.permute([1,0,2])\n",
    "        x=x[-1]\n",
    "        x=self.fcmodel(x)\n",
    "        return x\n",
    "\n",
    "class PhonemeClassifier(object):\n",
    "    def __init__(self):\n",
    "        unique_phonemes = ['CL', 'SF', 'VS', 'WF', 'ST', 'NF', \"q\"]\n",
    "        self.labels = unique_phonemes\n",
    "\n",
    "    def label_to_ids(self, y):\n",
    "        y_ = [self.labels.index(label) for label in y]\n",
    "        return y_\n",
    "\n",
    "    def id_to_label(self, y):\n",
    "        y_ = [self.labels[i] for i in y]\n",
    "        return y_\n",
    "        \n",
    "    def train(self, X_train, y_train):\n",
    "        y_train = self.label_to_ids(y_train)\n",
    "        y_train = np.asarray(y_train)\n",
    "        ### BEGIN YOUR CODE\n",
    "        X_train=torch.from_numpy(X_train).float()\n",
    "        X_train=X_train.reshape((-1,13,3))\n",
    "        y_train=torch.Tensor(y_train).long()\n",
    "        print(X_train.shape,y_train.shape)\n",
    "        dataset=TensorDataset(X_train,y_train)\n",
    "        dataset=DataLoader(dataset,batch_size=4096,shuffle=True)\n",
    "        device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model=Net()\n",
    "        model=model.to(device)\n",
    "        loss_fn=nn.CrossEntropyLoss()\n",
    "        loss_fn=loss_fn.to(device)\n",
    "        optimizer=torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "        model.train()\n",
    "        for epoch in range(20):\n",
    "            loss_sum=0\n",
    "            for data in dataset:\n",
    "                x,y=data\n",
    "                x=x.to(device)\n",
    "                y=y.to(device)\n",
    "                y_pred=model(x)\n",
    "                loss=loss_fn(y_pred,y)\n",
    "                loss_sum+=loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(\"Epoch: {}, Loss_sum: {}\".format(epoch,loss_sum))\n",
    "        ### END YOUR CODE\n",
    "        self.model = model\n",
    "\n",
    "    def test(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        @arguments:\n",
    "        X_test: #frames, #features (39 for mfcc)\n",
    "        y_test: frame labels\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE\n",
    "        # perform prediction and get out_classes array\n",
    "        # which contain class label id for each frame.\n",
    "        X_test=torch.from_numpy(X_test).float()\n",
    "        X_test=X_test.reshape((-1,13,3))\n",
    "        device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        X_test=X_test.to(device)\n",
    "        self.model.eval()\n",
    "        label_predict=self.model(X_test)\n",
    "        out_classes=torch.argmax(label_predict,axis=1)\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        out_classes = self.id_to_label(out_classes) # from id to string\n",
    "        out_classes = np.asarray(out_classes)\n",
    "        acc = sum(out_classes == y_test) * 1.0 / len(out_classes)\n",
    "        print('Frame-based Accuracy using %d testing samples: %f' % (X_test.shape[0], acc))\n",
    "\n",
    "        return out_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b961a843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from submission import model\n",
    "\n",
    "wav_files, label_files = read_data_folder(\"audio/part-2/train\")\n",
    "X_train, y_train, _, _  = prepare_data(wav_files, label_files)\n",
    "X_train, scaler_mean, scaler_std= normalize_mean(X_train)\n",
    "wav_files, label_files = read_data_folder(\"audio/part-2/val\")\n",
    "print (len(wav_files), len(label_files))\n",
    "X_test, y_test, test_seg_ids, test_seg2labels  = prepare_data(\n",
    "        wav_files, label_files)\n",
    "\n",
    "X_test = apply_normalize_mean(X_test, scaler_mean,scaler_std)\n",
    "cls = model.PhonemeClassifier()\n",
    "cls.train(X_train, y_train)\n",
    "y_pred = cls.test(X_test, y_test)\n",
    "\n",
    "model.segment_based_evaluation(y_pred, test_seg_ids, test_seg2labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f11e6f82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 342/342 [00:56<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:14<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74923, 13, 3]) torch.Size([74923])\n",
      "Epoch: 0, Loss_sum: 31.466890335083008\n",
      "Epoch: 1, Loss_sum: 28.98883056640625\n",
      "Epoch: 2, Loss_sum: 28.808786392211914\n",
      "Epoch: 3, Loss_sum: 28.703929901123047\n",
      "Epoch: 4, Loss_sum: 28.651626586914062\n",
      "Epoch: 5, Loss_sum: 28.679304122924805\n",
      "Epoch: 6, Loss_sum: 28.591684341430664\n",
      "Epoch: 7, Loss_sum: 28.58216667175293\n",
      "Epoch: 8, Loss_sum: 28.600013732910156\n",
      "Epoch: 9, Loss_sum: 28.54745101928711\n",
      "Epoch: 10, Loss_sum: 28.551931381225586\n",
      "Epoch: 11, Loss_sum: 28.54212188720703\n",
      "Epoch: 12, Loss_sum: 28.545263290405273\n",
      "Epoch: 13, Loss_sum: 28.50725555419922\n",
      "Epoch: 14, Loss_sum: 28.51063346862793\n",
      "Epoch: 15, Loss_sum: 28.51105499267578\n",
      "Epoch: 16, Loss_sum: 28.469661712646484\n",
      "Epoch: 17, Loss_sum: 28.413740158081055\n",
      "Epoch: 18, Loss_sum: 28.174589157104492\n",
      "Epoch: 19, Loss_sum: 27.88580894470215\n",
      "Frame-based Accuracy using 19258 testing samples: 0.513761\n",
      "Segment-based Accuracy using 3324 testing samples: 0.436823\n"
     ]
    }
   ],
   "source": [
    "%%writefile submission/best_model.py\n",
    "from submission import model\n",
    "#from submission import dataset as ds\n",
    "from submission import features as ft\n",
    "\n",
    "class BestAnalysis(ft.MFCCAnalysis):\n",
    "    def perform(self,wavfile):\n",
    "        ### BEGIN YOUR CODE\n",
    "        ##\n",
    "        x, sampling_rate = librosa.load(wavfile)\n",
    "        window_duration_ms = 40\n",
    "        n_fft = int((window_duration_ms / 1000.) * sampling_rate)\n",
    "\n",
    "        hop_duration_ms = 10\n",
    "        hop_length = int((hop_duration_ms / 1000.) * sampling_rate)\n",
    "        mfcc_count = 13\n",
    "        mfcc=librosa.feature.mfcc(x,sr=sampling_rate,n_mfcc=mfcc_count,n_fft=n_fft,hop_length=hop_length)\n",
    "        mfccs_and_deltas=mfcc\n",
    "        mfcc_delta1=librosa.feature.delta(mfcc,order=1)\n",
    "        mfccs_and_deltas=np.concatenate((mfccs_and_deltas,mfcc_delta1),axis=0)\n",
    "        mfcc_delta2=librosa.feature.delta(mfcc,order=2)\n",
    "        mfccs_and_deltas=np.concatenate((mfccs_and_deltas,mfcc_delta2),axis=0)\n",
    "        mfccs_and_deltas=mfccs_and_deltas.transpose(1,0)\n",
    "        ##\n",
    "        #### END YOUR CODE\n",
    "        return mfccs_and_deltas, hop_length, n_fft\n",
    "        \n",
    "    \n",
    "wav_files, label_files = read_data_folder(\"audio/part-2/train\")\n",
    "X_train, y_train, _, _  = prepare_data(wav_files, label_files, stanalysis=BestAnalysis())\n",
    "\n",
    "X_train, scaler_mean ,scaler_std= normalize_mean(X_train)\n",
    "    \n",
    "wav_files, label_files = read_data_folder(\"audio/part-2/val\")\n",
    "print (len(wav_files), len(label_files))\n",
    "X_test, y_test, test_seg_ids, test_seg2labels  = prepare_data(\n",
    "        wav_files, label_files, stanalysis=BestAnalysis())\n",
    "\n",
    "X_test = apply_normalize_mean(X_test, scaler_mean,scaler_std)\n",
    "cls = model.PhonemeClassifier()\n",
    "cls.train(X_train, y_train)\n",
    "y_pred = cls.test(X_test, y_test)\n",
    "\n",
    "model.segment_based_evaluation(y_pred, test_seg_ids, test_seg2labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c901773",
   "metadata": {},
   "source": [
    "## VI. Written Report (15%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7836e40",
   "metadata": {},
   "source": [
    "### Question 1 (3%):\n",
    "\n",
    "Describe your classification model.\n",
    "\n",
    "**Your answer**\n",
    "At first, I convert the input from shape (1,39) to (3,13), then put it through a LSTM (hidden_size=64), then I concatenate it with three linear layer with shape 64*64, 64*32, 32*7 respectively. (Activation funciton is ReLU, Dropout=0.5).\n",
    "### Question 2 (2%)\n",
    "What is the best performance that you can get with MFCC on the validation set?\n",
    "\n",
    "**Your answer**\n",
    "Frame-based Accuracy: 0.518901\n",
    "Segment-based Accuracy: 0.438327\n",
    "\n",
    "### Question 3 (10%): \n",
    "\n",
    "Discribe methods that you have tried and the results that you have achived when you worked on the \"best model\" task\n",
    "\n",
    "**Your answer**\n",
    "I have tried to use only fully connected layers, LSTMS, CNNs, or the combinations of them, but it won't yeild better result because of over-fitting problem. I also tried differnt hop_length and window_length, but it usually results in trade-off problem (longer window_length gives back better frame-based accuracy but worser segment-based accuracy). More features, different optimizers and learning rates also feed back similiar results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc978703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
